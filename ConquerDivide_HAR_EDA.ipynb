{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DSnP2PJNznkv"
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,CuDNNLSTM,BatchNormalization\n",
    "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RSynS4Nw9et7"
   },
   "source": [
    "## Import File from local drive to Colab Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Xwve5jBPP0za",
    "outputId": "76d9fc0f-b728-42e1-d6ce-b6ca8e825dee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldi8ORcpP5NH"
   },
   "source": [
    "## Load the Signals (Input Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pp0PF-8sQKMc"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "def load_data(incoming):\n",
    "  path=\"drive/My Drive/Data/Human_activity_recognition\" +'/'+ incoming\n",
    "  print(path)\n",
    "  files=os.listdir(path)\n",
    "  print(files)\n",
    "  signals_data = []\n",
    "\n",
    "  for file in files:\n",
    "    filename=path+'/'+file\n",
    "    df=pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "    df=(df-df.mean())/(df.max()-df.min())\n",
    "    signals_data.append(df.as_matrix())\n",
    "    \n",
    "  # Transpose is used to change the dimensionality of the output,\n",
    "  # aggregating the signals by combination of sample/timestep.\n",
    "  # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "  return np.transpose(signals_data, (1, 2, 0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "MNbE0YLFRPoJ",
    "outputId": "8603e02f-b353-42fe-e5e4-2c0968f12aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive/My Drive/Data/Human_activity_recognition/Train\n",
      "['body_acc_x_train.txt', 'body_acc_y_train.txt', 'body_acc_z_train.txt', 'body_gyro_x_train.txt', 'body_gyro_y_train.txt', 'body_gyro_z_train.txt', 'total_acc_x_train.txt', 'total_acc_y_train.txt', 'total_acc_z_train.txt']\n",
      "drive/My Drive/Data/Human_activity_recognition/Test\n",
      "['body_acc_x_test.txt', 'body_acc_y_test.txt', 'body_acc_z_test.txt', 'body_gyro_x_test.txt', 'body_gyro_y_test.txt', 'body_gyro_z_test.txt', 'total_acc_x_test.txt', 'total_acc_y_test.txt', 'total_acc_z_test.txt']\n",
      "(7352, 128, 9)\n",
      "(2947, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "X_train = load_data('Train')\n",
    "X_test = load_data('Test')\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iebIO2V16_E5"
   },
   "outputs": [],
   "source": [
    "def load_op(incoming):\n",
    "  \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "  path=\"drive/My Drive/Data/Human_activity_recognition\" +'/'+ incoming +'.txt'\n",
    "  \n",
    "  df=pd.read_csv(path, delim_whitespace=True, header=None)[0]\n",
    "  \n",
    "  return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UstDrla47BlF"
   },
   "outputs": [],
   "source": [
    "y_train=load_op('y_train')\n",
    "y_test=load_op('y_test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RokpWzLcyag2"
   },
   "source": [
    "## Loading the Output labels bt spliting into Static and Dynamic \n",
    "\n",
    "\n",
    "1.  walking, up, down -- dynamic\n",
    "2.   sitting standing lying -- static \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1n6iKbQWUSy3"
   },
   "outputs": [],
   "source": [
    "def load_op_2(incoming):\n",
    "  \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "  path=\"drive/My Drive/Data/Human_activity_recognition\" +'/'+ incoming +'.txt'\n",
    "  \n",
    "  df=pd.read_csv(path, delim_whitespace=True, header=None)[0]\n",
    "  df[df<=3] = 0\n",
    "  df[df>3] = 1\n",
    "  return pd.get_dummies(df).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "72f-1DyYy0A4",
    "outputId": "63896358-6e98-46ca-ce76-1b8f23eed758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 2)\n",
      "(2947, 2)\n"
     ]
    }
   ],
   "source": [
    "y_train_2=load_op_2('y_train')\n",
    "y_test_2=load_op_2('y_test')\n",
    "print(y_train_2.shape)\n",
    "print(y_test_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqYLCV38zrVv"
   },
   "source": [
    "## Model for classifying data into Static and Dynamic activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "cj9WtMDczwWf",
    "outputId": "7090badc-05ce-4a92-a982-f8576ef7bf39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0630 12:30:40.393746 140271284651904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0630 12:30:40.411869 140271284651904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0630 12:30:40.415036 140271284651904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0630 12:30:40.459711 140271284651904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0630 12:30:40.471667 140271284651904 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0630 12:30:40.472936 140271284651904 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0630 12:30:40.496779 140271284651904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 126, 32)           896       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 124, 32)           3104      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 124, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 62, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1984)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                99250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 103,352\n",
      "Trainable params: 103,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "colab_type": "code",
    "id": "9dzDWqTt0Ajc",
    "outputId": "bdab8b55-77ee-4f91-d306-ac39a5a42d59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0630 12:30:40.560046 140271284651904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0630 12:30:40.594328 140271284651904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0630 12:30:40.712684 140271284651904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/10\n",
      "7352/7352 [==============================] - 5s 693us/step - loss: 0.0323 - acc: 0.9846 - val_loss: 0.0091 - val_acc: 0.9980\n",
      "Epoch 2/10\n",
      "7352/7352 [==============================] - 3s 446us/step - loss: 1.9257e-04 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 0.9973\n",
      "Epoch 3/10\n",
      "7352/7352 [==============================] - 3s 449us/step - loss: 9.1948e-05 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 0.9983\n",
      "Epoch 4/10\n",
      "7352/7352 [==============================] - 3s 448us/step - loss: 1.1764e-04 - acc: 1.0000 - val_loss: 0.0044 - val_acc: 0.9990\n",
      "Epoch 5/10\n",
      "7352/7352 [==============================] - 3s 444us/step - loss: 3.2361e-05 - acc: 1.0000 - val_loss: 0.0090 - val_acc: 0.9983\n",
      "Epoch 6/10\n",
      "7352/7352 [==============================] - 3s 451us/step - loss: 1.4335e-05 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 0.9986\n",
      "Epoch 7/10\n",
      "7352/7352 [==============================] - 3s 446us/step - loss: 2.8019e-06 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 0.9986\n",
      "Epoch 8/10\n",
      "7352/7352 [==============================] - 3s 442us/step - loss: 1.9920e-06 - acc: 1.0000 - val_loss: 0.0074 - val_acc: 0.9986\n",
      "Epoch 9/10\n",
      "7352/7352 [==============================] - 3s 452us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0060 - val_acc: 0.9983\n",
      "Epoch 10/10\n",
      "7352/7352 [==============================] - 3s 446us/step - loss: 1.5234e-05 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f93222bc828>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train_2, epochs=10, batch_size=16,validation_data=(X_test, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k23RYA0l0QEW"
   },
   "source": [
    "## Save the 2 class classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lzw4p2Qp0doc"
   },
   "outputs": [],
   "source": [
    "model.save('drive/My Drive/Data/Human_activity_recognition/model_2class.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nP87LG0u0j3j"
   },
   "source": [
    "## Classificaton of Static activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PsmWpEH70tHZ"
   },
   "outputs": [],
   "source": [
    "def load_op_stat(incoming):\n",
    "  \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "  path=\"drive/My Drive/Data/Human_activity_recognition\" +'/'+ incoming +'.txt'\n",
    "  \n",
    "  df=pd.read_csv(path, delim_whitespace=True, header=None)[0]\n",
    "  df_subset=df>3\n",
    "  df=df[df_subset]\n",
    "  return pd.get_dummies(df).as_matrix(),df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "vLyPAIxo2RxP",
    "outputId": "9fe0aad5-e77c-402a-9a1b-cd2f645b9831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4067, 3)\n",
      "(1560, 3)\n",
      "(1560, 128, 9)\n",
      "(4067, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "y_train_stat,x_train_size=load_op_stat('y_train')\n",
    "y_test_stat,x_test_size=load_op_stat('y_test')\n",
    "X_train_stat=X_train[x_train_size]\n",
    "X_test_stat=X_test[x_test_size]\n",
    "print(y_train_stat.shape)\n",
    "print(y_test_stat.shape)\n",
    "print(X_test_stat.shape)\n",
    "print(X_train_stat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YOI4XzkF2Xj_"
   },
   "source": [
    "## Model for Static Activites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UjizrgTE5tiA"
   },
   "outputs": [],
   "source": [
    "acc_score=[]\n",
    "for f in [32,64]:\n",
    "  for f1 in [16,32]:\n",
    "    for k in [5,3]:\n",
    "      for k1 in [5,3]:\n",
    "        for d in [0.5,0.6]:\n",
    "          for p in [2,1]:\n",
    "            for s in [2,1]:\n",
    "\n",
    "              model = Sequential()\n",
    "              model.add(Conv1D(filters=f, kernel_size=k,padding='same', activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "              model.add(Conv1D(filters=f1, kernel_size=k1,padding='same', activation='relu',kernel_initializer='he_uniform'))\n",
    "              model.add(Dropout(d))\n",
    "              model.add(MaxPooling1D(pool_size=p,strides=s))\n",
    "\n",
    "              model.add(Flatten())\n",
    "              model.add(Dense(64, activation='relu',kernel_initializer='he_uniform'))\n",
    "              model.add(BatchNormalization()) \n",
    "              model.add(Dropout(d))\n",
    "\n",
    "              model.add(Dense(32, activation='relu',kernel_initializer='he_uniform'))\n",
    "              model.add(BatchNormalization()) \n",
    "              model.add(Dropout(d))\n",
    "\n",
    "              model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "              model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "              model.fit(X_train_stat,y_train_stat, epochs=10, batch_size=16,validation_data=(X_test_stat, y_test_stat))\n",
    "\n",
    "              #Evaluate the model \n",
    "              score = model.evaluate(X_test_stat, y_test_stat)\n",
    "\n",
    "              #Add the evaluation to a list \n",
    "              acc_score.append([f,f1,k,k1,d,p,s,score[1]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gtHeC0F66Cbs",
    "outputId": "ac2ce3b0-8e69-4471-e80b-12601168c758"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_1</th>\n",
       "      <th>conv_2</th>\n",
       "      <th>kernel1</th>\n",
       "      <th>kernel2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>poolsize</th>\n",
       "      <th>stride</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.894872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.871795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.887821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.864103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.894231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.878205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.879487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.860897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.896795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.862821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.867949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.887179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.869872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.865385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.885256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.888462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.881410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.878846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.892308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.891667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.886538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.880128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.888462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.864744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.879487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.866026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.858974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.882692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.886538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.899359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.887179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.901282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.880128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.889744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.907692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.883974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.877564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.883974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.891026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.889103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.889744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.861538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.892949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.891667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.891667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.874359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.892949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.887821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     conv_1  conv_2  kernel1  kernel2  dropout  poolsize  stride  accuracy\n",
       "0        32      16        5        5      0.5         2       2  0.894872\n",
       "1        32      16        5        5      0.5         2       1  0.900641\n",
       "2        32      16        5        5      0.5         1       2  0.871795\n",
       "3        32      16        5        5      0.5         1       1  0.887821\n",
       "4        32      16        5        5      0.6         2       2  0.864103\n",
       "5        32      16        5        5      0.6         2       1  0.885897\n",
       "6        32      16        5        5      0.6         1       2  0.894231\n",
       "7        32      16        5        5      0.6         1       1  0.878205\n",
       "8        32      16        5        3      0.5         2       2  0.879487\n",
       "9        32      16        5        3      0.5         2       1  0.860897\n",
       "10       32      16        5        3      0.5         1       2  0.896795\n",
       "11       32      16        5        3      0.5         1       1  0.876923\n",
       "12       32      16        5        3      0.6         2       2  0.862821\n",
       "13       32      16        5        3      0.6         2       1  0.867949\n",
       "14       32      16        5        3      0.6         1       2  0.887179\n",
       "15       32      16        5        3      0.6         1       1  0.898077\n",
       "16       32      16        3        5      0.5         2       2  0.869872\n",
       "17       32      16        3        5      0.5         2       1  0.865385\n",
       "18       32      16        3        5      0.5         1       2  0.885256\n",
       "19       32      16        3        5      0.5         1       1  0.888462\n",
       "20       32      16        3        5      0.6         2       2  0.881410\n",
       "21       32      16        3        5      0.6         2       1  0.878846\n",
       "22       32      16        3        5      0.6         1       2  0.892308\n",
       "23       32      16        3        5      0.6         1       1  0.883333\n",
       "24       32      16        3        3      0.5         2       2  0.891667\n",
       "25       32      16        3        3      0.5         2       1  0.880769\n",
       "26       32      16        3        3      0.5         1       2  0.886538\n",
       "27       32      16        3        3      0.5         1       1  0.880128\n",
       "28       32      16        3        3      0.6         2       2  0.888462\n",
       "29       32      16        3        3      0.6         2       1  0.864744\n",
       "..      ...     ...      ...      ...      ...       ...     ...       ...\n",
       "98       64      32        5        5      0.5         1       2  0.879487\n",
       "99       64      32        5        5      0.5         1       1  0.866026\n",
       "100      64      32        5        5      0.6         2       2  0.858974\n",
       "101      64      32        5        5      0.6         2       1  0.882692\n",
       "102      64      32        5        5      0.6         1       2  0.886538\n",
       "103      64      32        5        5      0.6         1       1  0.899359\n",
       "104      64      32        5        3      0.5         2       2  0.887179\n",
       "105      64      32        5        3      0.5         2       1  0.869231\n",
       "106      64      32        5        3      0.5         1       2  0.901282\n",
       "107      64      32        5        3      0.5         1       1  0.883333\n",
       "108      64      32        5        3      0.6         2       2  0.880128\n",
       "109      64      32        5        3      0.6         2       1  0.876282\n",
       "110      64      32        5        3      0.6         1       2  0.889744\n",
       "111      64      32        5        3      0.6         1       1  0.907692\n",
       "112      64      32        3        5      0.5         2       2  0.883974\n",
       "113      64      32        3        5      0.5         2       1  0.877564\n",
       "114      64      32        3        5      0.5         1       2  0.883974\n",
       "115      64      32        3        5      0.5         1       1  0.875000\n",
       "116      64      32        3        5      0.6         2       2  0.891026\n",
       "117      64      32        3        5      0.6         2       1  0.889103\n",
       "118      64      32        3        5      0.6         1       2  0.889744\n",
       "119      64      32        3        5      0.6         1       1  0.875641\n",
       "120      64      32        3        3      0.5         2       2  0.861538\n",
       "121      64      32        3        3      0.5         2       1  0.892949\n",
       "122      64      32        3        3      0.5         1       2  0.891667\n",
       "123      64      32        3        3      0.5         1       1  0.876282\n",
       "124      64      32        3        3      0.6         2       2  0.891667\n",
       "125      64      32        3        3      0.6         2       1  0.874359\n",
       "126      64      32        3        3      0.6         1       2  0.892949\n",
       "127      64      32        3        3      0.6         1       1  0.887821\n",
       "\n",
       "[128 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(acc_score,columns=['conv_1','conv_2','kernel1','kernel2','dropout','poolsize','stride','accuracy'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "pc1s-VWZD86P",
    "outputId": "5700b344-9837-4d33-aa3d-87ccdf6f594f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_1</th>\n",
       "      <th>conv_2</th>\n",
       "      <th>kernel1</th>\n",
       "      <th>kernel2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>poolsize</th>\n",
       "      <th>stride</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.907692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     conv_1  conv_2  kernel1  kernel2  dropout  poolsize  stride  accuracy\n",
       "111      64      32        5        3      0.6         1       1  0.907692"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['accuracy']==df['accuracy'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "9yJ7Jcqt3VFv",
    "outputId": "da3f3fa8-cbc7-440e-855b-c85fc4a68f23"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_1</th>\n",
       "      <th>conv_2</th>\n",
       "      <th>kernel1</th>\n",
       "      <th>kernel2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>poolsize</th>\n",
       "      <th>stride</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.907692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.901923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.901282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.900641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.899359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.898718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     conv_1  conv_2  kernel1  kernel2  dropout  poolsize  stride  accuracy\n",
       "111      64      32        5        3      0.6         1       1  0.907692\n",
       "47       32      32        5        3      0.6         1       1  0.903846\n",
       "42       32      32        5        3      0.5         1       2  0.901923\n",
       "106      64      32        5        3      0.5         1       2  0.901282\n",
       "32       32      32        5        5      0.5         2       2  0.900641\n",
       "1        32      16        5        5      0.5         2       1  0.900641\n",
       "67       64      16        5        5      0.5         1       1  0.900000\n",
       "103      64      32        5        5      0.6         1       1  0.899359\n",
       "49       32      32        3        5      0.5         2       1  0.898718\n",
       "66       64      16        5        5      0.5         1       2  0.898718"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('accuracy',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hhoXXBMN3buK",
    "outputId": "21e62577-c932-41bf-e2be-d16387509b2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4067 samples, validate on 1560 samples\n",
      "Epoch 1/100\n",
      "4067/4067 [==============================] - 36s 9ms/step - loss: 0.4880 - acc: 0.8117 - val_loss: 0.3385 - val_acc: 0.8718\n",
      "Epoch 2/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.3305 - acc: 0.8817 - val_loss: 0.3655 - val_acc: 0.8923\n",
      "Epoch 3/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2840 - acc: 0.8965 - val_loss: 0.3409 - val_acc: 0.8686\n",
      "Epoch 4/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2663 - acc: 0.9007 - val_loss: 0.2910 - val_acc: 0.8942\n",
      "Epoch 5/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2475 - acc: 0.9093 - val_loss: 0.2989 - val_acc: 0.8872\n",
      "Epoch 6/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2550 - acc: 0.9058 - val_loss: 0.2924 - val_acc: 0.8949\n",
      "Epoch 7/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2435 - acc: 0.9100 - val_loss: 0.3254 - val_acc: 0.8885\n",
      "Epoch 8/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2234 - acc: 0.9134 - val_loss: 0.3108 - val_acc: 0.8910\n",
      "Epoch 9/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2161 - acc: 0.9176 - val_loss: 0.3058 - val_acc: 0.8949\n",
      "Epoch 10/100\n",
      "4067/4067 [==============================] - 6s 2ms/step - loss: 0.2185 - acc: 0.9225 - val_loss: 0.3331 - val_acc: 0.8724\n",
      "Epoch 11/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2006 - acc: 0.9292 - val_loss: 0.3167 - val_acc: 0.8763\n",
      "Epoch 12/100\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1906 - acc: 0.9267 - val_loss: 0.3325 - val_acc: 0.8872\n",
      "Epoch 13/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1984 - acc: 0.9326 - val_loss: 0.2942 - val_acc: 0.9051\n",
      "Epoch 14/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1863 - acc: 0.9329 - val_loss: 0.4199 - val_acc: 0.8404\n",
      "Epoch 15/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2136 - acc: 0.9260 - val_loss: 0.2892 - val_acc: 0.9071\n",
      "Epoch 16/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1886 - acc: 0.9373 - val_loss: 0.3161 - val_acc: 0.9064\n",
      "Epoch 17/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1924 - acc: 0.9346 - val_loss: 0.2911 - val_acc: 0.9083\n",
      "Epoch 18/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1837 - acc: 0.9373 - val_loss: 0.3583 - val_acc: 0.8865\n",
      "Epoch 19/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1698 - acc: 0.9442 - val_loss: 0.2925 - val_acc: 0.9096\n",
      "Epoch 20/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.2028 - acc: 0.9351 - val_loss: 0.3093 - val_acc: 0.8923\n",
      "Epoch 21/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1831 - acc: 0.9366 - val_loss: 0.2938 - val_acc: 0.9038\n",
      "Epoch 22/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1869 - acc: 0.9407 - val_loss: 0.3037 - val_acc: 0.9147\n",
      "Epoch 23/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1593 - acc: 0.9444 - val_loss: 0.2965 - val_acc: 0.9147\n",
      "Epoch 24/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1728 - acc: 0.9415 - val_loss: 0.2649 - val_acc: 0.9096\n",
      "Epoch 25/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1764 - acc: 0.9417 - val_loss: 0.3241 - val_acc: 0.9115\n",
      "Epoch 26/100\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1803 - acc: 0.9395 - val_loss: 0.3314 - val_acc: 0.8968\n",
      "Epoch 27/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1515 - acc: 0.9484 - val_loss: 0.3371 - val_acc: 0.8923\n",
      "Epoch 28/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1593 - acc: 0.9479 - val_loss: 0.3607 - val_acc: 0.8878\n",
      "Epoch 29/100\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1604 - acc: 0.9464 - val_loss: 0.3442 - val_acc: 0.8923\n",
      "Epoch 30/100\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1742 - acc: 0.9464 - val_loss: 0.2789 - val_acc: 0.9147\n",
      "Epoch 31/100\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1518 - acc: 0.9513 - val_loss: 0.3326 - val_acc: 0.9071\n",
      "Epoch 32/100\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1780 - acc: 0.9474 - val_loss: 0.2777 - val_acc: 0.9103\n",
      "Epoch 33/100\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1856 - acc: 0.9452 - val_loss: 0.3808 - val_acc: 0.8891\n",
      "Epoch 34/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1777 - acc: 0.9447 - val_loss: 0.3110 - val_acc: 0.8987\n",
      "Epoch 35/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1566 - acc: 0.9503 - val_loss: 0.3206 - val_acc: 0.9192\n",
      "Epoch 36/100\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1551 - acc: 0.9513 - val_loss: 0.3812 - val_acc: 0.8853\n",
      "Epoch 37/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1553 - acc: 0.9479 - val_loss: 0.2886 - val_acc: 0.9160\n",
      "Epoch 38/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1345 - acc: 0.9516 - val_loss: 0.3683 - val_acc: 0.8904\n",
      "Epoch 39/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1370 - acc: 0.9528 - val_loss: 0.3758 - val_acc: 0.8994\n",
      "Epoch 40/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1755 - acc: 0.9491 - val_loss: 0.2802 - val_acc: 0.9109\n",
      "Epoch 41/100\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1320 - acc: 0.9562 - val_loss: 0.4842 - val_acc: 0.8603\n",
      "Epoch 42/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1719 - acc: 0.9493 - val_loss: 0.5051 - val_acc: 0.8660\n",
      "Epoch 43/100\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1426 - acc: 0.9552 - val_loss: 0.3621 - val_acc: 0.8949\n",
      "Epoch 44/100\n",
      "4067/4067 [==============================] - 6s 1ms/step - loss: 0.1602 - acc: 0.9533 - val_loss: 0.3533 - val_acc: 0.8853\n",
      "Epoch 45/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1468 - acc: 0.9535 - val_loss: 0.3160 - val_acc: 0.9160\n",
      "Epoch 46/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1579 - acc: 0.9550 - val_loss: 0.3248 - val_acc: 0.9128\n",
      "Epoch 47/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1419 - acc: 0.9538 - val_loss: 0.3502 - val_acc: 0.9135\n",
      "Epoch 48/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1337 - acc: 0.9552 - val_loss: 0.4048 - val_acc: 0.8846\n",
      "Epoch 49/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1487 - acc: 0.9518 - val_loss: 0.3291 - val_acc: 0.8994\n",
      "Epoch 50/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1530 - acc: 0.9533 - val_loss: 0.4015 - val_acc: 0.8929\n",
      "Epoch 51/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1301 - acc: 0.9580 - val_loss: 0.3403 - val_acc: 0.9071\n",
      "Epoch 52/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1475 - acc: 0.9567 - val_loss: 0.3983 - val_acc: 0.8801\n",
      "Epoch 53/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1317 - acc: 0.9594 - val_loss: 0.4154 - val_acc: 0.8801\n",
      "Epoch 54/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1262 - acc: 0.9604 - val_loss: 0.3428 - val_acc: 0.9135\n",
      "Epoch 55/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1368 - acc: 0.9577 - val_loss: 0.4290 - val_acc: 0.8840\n",
      "Epoch 56/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1396 - acc: 0.9577 - val_loss: 0.4651 - val_acc: 0.8731\n",
      "Epoch 57/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1539 - acc: 0.9562 - val_loss: 0.4205 - val_acc: 0.8891\n",
      "Epoch 58/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1334 - acc: 0.9570 - val_loss: 0.4869 - val_acc: 0.8821\n",
      "Epoch 59/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1367 - acc: 0.9602 - val_loss: 0.5119 - val_acc: 0.8763\n",
      "Epoch 60/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1418 - acc: 0.9584 - val_loss: 0.4014 - val_acc: 0.8974\n",
      "Epoch 61/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1494 - acc: 0.9552 - val_loss: 0.3167 - val_acc: 0.9154\n",
      "Epoch 62/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1467 - acc: 0.9567 - val_loss: 0.3295 - val_acc: 0.9064\n",
      "Epoch 63/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1398 - acc: 0.9562 - val_loss: 0.4167 - val_acc: 0.8942\n",
      "Epoch 64/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1266 - acc: 0.9575 - val_loss: 0.3457 - val_acc: 0.9083\n",
      "Epoch 65/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1210 - acc: 0.9614 - val_loss: 0.3324 - val_acc: 0.9186\n",
      "Epoch 66/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1186 - acc: 0.9643 - val_loss: 0.3614 - val_acc: 0.9090\n",
      "Epoch 67/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1208 - acc: 0.9671 - val_loss: 0.4303 - val_acc: 0.8910\n",
      "Epoch 68/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1411 - acc: 0.9584 - val_loss: 0.3359 - val_acc: 0.9179\n",
      "Epoch 69/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1106 - acc: 0.9685 - val_loss: 0.3972 - val_acc: 0.9077\n",
      "Epoch 70/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1046 - acc: 0.9680 - val_loss: 0.3858 - val_acc: 0.9096\n",
      "Epoch 71/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1373 - acc: 0.9587 - val_loss: 0.3809 - val_acc: 0.9147\n",
      "Epoch 72/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1286 - acc: 0.9641 - val_loss: 0.4533 - val_acc: 0.8968\n",
      "Epoch 73/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1304 - acc: 0.9626 - val_loss: 0.4530 - val_acc: 0.9013\n",
      "Epoch 74/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1256 - acc: 0.9629 - val_loss: 0.4022 - val_acc: 0.9128\n",
      "Epoch 75/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1297 - acc: 0.9666 - val_loss: 0.4440 - val_acc: 0.8994\n",
      "Epoch 76/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1057 - acc: 0.9656 - val_loss: 0.4379 - val_acc: 0.9019\n",
      "Epoch 77/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1011 - acc: 0.9717 - val_loss: 0.3996 - val_acc: 0.9090\n",
      "Epoch 78/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1047 - acc: 0.9688 - val_loss: 0.4963 - val_acc: 0.9045\n",
      "Epoch 79/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1299 - acc: 0.9641 - val_loss: 0.6195 - val_acc: 0.8904\n",
      "Epoch 80/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0999 - acc: 0.9705 - val_loss: 0.4580 - val_acc: 0.9167\n",
      "Epoch 81/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0946 - acc: 0.9739 - val_loss: 0.5952 - val_acc: 0.9013\n",
      "Epoch 82/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1047 - acc: 0.9717 - val_loss: 0.4342 - val_acc: 0.9058\n",
      "Epoch 83/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1047 - acc: 0.9671 - val_loss: 0.4959 - val_acc: 0.9122\n",
      "Epoch 84/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1016 - acc: 0.9700 - val_loss: 0.4871 - val_acc: 0.9026\n",
      "Epoch 85/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0878 - acc: 0.9725 - val_loss: 0.4597 - val_acc: 0.9077\n",
      "Epoch 86/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0947 - acc: 0.9727 - val_loss: 0.5689 - val_acc: 0.8981\n",
      "Epoch 87/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1081 - acc: 0.9715 - val_loss: 0.5354 - val_acc: 0.9058\n",
      "Epoch 88/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1076 - acc: 0.9698 - val_loss: 0.4574 - val_acc: 0.9090\n",
      "Epoch 89/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1004 - acc: 0.9712 - val_loss: 0.6170 - val_acc: 0.9006\n",
      "Epoch 90/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1087 - acc: 0.9683 - val_loss: 0.7206 - val_acc: 0.8724\n",
      "Epoch 91/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0820 - acc: 0.9747 - val_loss: 0.5461 - val_acc: 0.9019\n",
      "Epoch 92/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0786 - acc: 0.9757 - val_loss: 0.5071 - val_acc: 0.8955\n",
      "Epoch 93/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0905 - acc: 0.9754 - val_loss: 0.6025 - val_acc: 0.8974\n",
      "Epoch 94/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1049 - acc: 0.9702 - val_loss: 0.4002 - val_acc: 0.9090\n",
      "Epoch 95/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0906 - acc: 0.9739 - val_loss: 0.3941 - val_acc: 0.9179\n",
      "Epoch 96/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0771 - acc: 0.9759 - val_loss: 0.5847 - val_acc: 0.9032\n",
      "Epoch 97/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0853 - acc: 0.9752 - val_loss: 0.6068 - val_acc: 0.8974\n",
      "Epoch 98/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0828 - acc: 0.9759 - val_loss: 0.6733 - val_acc: 0.8994\n",
      "Epoch 99/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.1075 - acc: 0.9732 - val_loss: 0.5943 - val_acc: 0.8962\n",
      "Epoch 100/100\n",
      "4067/4067 [==============================] - 5s 1ms/step - loss: 0.0920 - acc: 0.9695 - val_loss: 0.5412 - val_acc: 0.9122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9269688d68>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  model = Sequential()\n",
    "  model.add(Conv1D(filters=64, kernel_size=5,padding='same', activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "  model.add(Conv1D(filters=32, kernel_size=3,padding='same', activation='relu',kernel_initializer='he_uniform'))\n",
    "  model.add(Dropout(0.6))\n",
    "  model.add(MaxPooling1D(pool_size=1,strides=1))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(64, activation='relu',kernel_initializer='he_uniform'))\n",
    "  model.add(BatchNormalization()) \n",
    "  model.add(Dropout(0.6))\n",
    "\n",
    "  model.add(Dense(32, activation='relu',kernel_initializer='he_uniform'))\n",
    "  model.add(BatchNormalization()) \n",
    "  model.add(Dropout(0.6))\n",
    "\n",
    "  model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "  model.fit(X_train_stat,y_train_stat, epochs=100, batch_size=16,validation_data=(X_test_stat, y_test_stat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQL_qxmI3t72"
   },
   "outputs": [],
   "source": [
    "model.save('drive/My Drive/Data/Human_activity_recognition/model_stat_class.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vj1mYSW09d8W"
   },
   "source": [
    "## For Dynamic Activities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsOP8V3W98AI"
   },
   "outputs": [],
   "source": [
    "def load_op_dyn(incoming):\n",
    "  \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "  path=\"drive/My Drive/Data/Human_activity_recognition\" +'/'+ incoming +'.txt'\n",
    "  \n",
    "  df=pd.read_csv(path, delim_whitespace=True, header=None)[0]\n",
    "  df_subset=df<=3\n",
    "  df=df[df_subset]\n",
    "  return pd.get_dummies(df).as_matrix(),df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "8EclCmsR98AM",
    "outputId": "c4c0a756-053e-4b34-b7e2-b6cb036bc820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3285, 3)\n",
      "(1387, 3)\n",
      "(1387, 128, 9)\n",
      "(3285, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "y_train_dyn,x_train_size=load_op_dyn('y_train')\n",
    "y_test_dyn,x_test_size=load_op_dyn('y_test')\n",
    "X_train_dyn=X_train[x_train_size]\n",
    "X_test_dyn=X_test[x_test_size]\n",
    "print(y_train_dyn.shape)\n",
    "print(y_test_dyn.shape)\n",
    "print(X_test_dyn.shape)\n",
    "print(X_train_dyn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7IS3w_2-QS5"
   },
   "source": [
    "## Model for Dynamic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Jz9zJd1D-o1t",
    "outputId": "480fd50e-3871-4b2e-e7c4-c492d4e3932e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387/1387 [==============================] - 0s 284us/step\n",
      "1387/1387 [==============================] - 0s 295us/step\n",
      "1387/1387 [==============================] - 0s 289us/step\n",
      "1387/1387 [==============================] - 0s 316us/step\n",
      "1387/1387 [==============================] - 0s 300us/step\n",
      "1387/1387 [==============================] - 0s 341us/step\n",
      "1387/1387 [==============================] - 0s 292us/step\n",
      "1387/1387 [==============================] - 0s 317us/step\n",
      "1387/1387 [==============================] - 0s 301us/step\n",
      "1387/1387 [==============================] - 0s 318us/step\n",
      "1387/1387 [==============================] - 0s 297us/step\n",
      "1387/1387 [==============================] - 0s 313us/step\n",
      "1387/1387 [==============================] - 0s 299us/step\n",
      "1387/1387 [==============================] - 0s 314us/step\n",
      "1387/1387 [==============================] - 0s 303us/step\n",
      "1387/1387 [==============================] - 0s 310us/step\n",
      "1387/1387 [==============================] - 0s 304us/step\n",
      "1387/1387 [==============================] - 0s 333us/step\n",
      "1387/1387 [==============================] - 0s 322us/step\n",
      "1387/1387 [==============================] - 0s 308us/step\n",
      "1387/1387 [==============================] - 0s 308us/step\n",
      "1387/1387 [==============================] - 0s 322us/step\n",
      "1387/1387 [==============================] - 0s 317us/step\n",
      "1387/1387 [==============================] - 0s 313us/step\n",
      "1387/1387 [==============================] - 0s 318us/step\n",
      "1387/1387 [==============================] - 0s 315us/step\n",
      "1387/1387 [==============================] - 0s 320us/step\n",
      "1387/1387 [==============================] - 0s 314us/step\n",
      "1387/1387 [==============================] - 0s 325us/step\n",
      "1387/1387 [==============================] - 0s 337us/step\n",
      "1387/1387 [==============================] - 0s 341us/step\n",
      "1387/1387 [==============================] - 0s 334us/step\n",
      "1387/1387 [==============================] - 0s 348us/step\n",
      "1387/1387 [==============================] - 0s 347us/step\n",
      "1387/1387 [==============================] - 0s 333us/step\n",
      "1387/1387 [==============================] - 0s 326us/step\n",
      "1387/1387 [==============================] - 0s 331us/step\n",
      "1387/1387 [==============================] - 0s 345us/step\n",
      "1387/1387 [==============================] - 0s 335us/step\n",
      "1387/1387 [==============================] - 0s 332us/step\n",
      "1387/1387 [==============================] - 1s 651us/step\n",
      "1387/1387 [==============================] - 1s 659us/step\n",
      "1387/1387 [==============================] - 1s 693us/step\n",
      "1387/1387 [==============================] - 1s 648us/step\n",
      "1387/1387 [==============================] - 1s 671us/step\n",
      "1387/1387 [==============================] - 1s 645us/step\n",
      "1387/1387 [==============================] - 1s 662us/step\n",
      "1387/1387 [==============================] - 1s 654us/step\n",
      "1387/1387 [==============================] - 1s 678us/step\n",
      "1387/1387 [==============================] - 1s 671us/step\n",
      "1387/1387 [==============================] - 1s 670us/step\n",
      "1387/1387 [==============================] - 1s 679us/step\n",
      "1387/1387 [==============================] - 1s 691us/step\n",
      "1387/1387 [==============================] - 1s 691us/step\n",
      "1387/1387 [==============================] - 1s 677us/step\n",
      "1387/1387 [==============================] - 1s 659us/step\n",
      "1387/1387 [==============================] - 1s 689us/step\n",
      "1387/1387 [==============================] - 1s 709us/step\n",
      "1387/1387 [==============================] - 1s 689us/step\n",
      "1387/1387 [==============================] - 1s 684us/step\n",
      "1387/1387 [==============================] - 1s 693us/step\n",
      "1387/1387 [==============================] - 1s 692us/step\n",
      "1387/1387 [==============================] - 1s 693us/step\n",
      "1387/1387 [==============================] - 1s 697us/step\n",
      "1387/1387 [==============================] - 1s 698us/step\n",
      "1387/1387 [==============================] - 1s 701us/step\n",
      "1387/1387 [==============================] - 1s 687us/step\n",
      "1387/1387 [==============================] - 1s 751us/step\n",
      "1387/1387 [==============================] - 1s 799us/step\n",
      "1387/1387 [==============================] - 1s 763us/step\n",
      "1387/1387 [==============================] - 1s 778us/step\n",
      "1387/1387 [==============================] - 1s 755us/step\n",
      "1387/1387 [==============================] - 1s 768us/step\n",
      "1387/1387 [==============================] - 1s 703us/step\n",
      "1387/1387 [==============================] - 1s 715us/step\n",
      "1387/1387 [==============================] - 1s 714us/step\n",
      "1387/1387 [==============================] - 1s 723us/step\n",
      "1387/1387 [==============================] - 1s 738us/step\n",
      "1387/1387 [==============================] - 1s 728us/step\n",
      "1387/1387 [==============================] - 1s 742us/step\n",
      "1387/1387 [==============================] - 1s 760us/step\n",
      "1387/1387 [==============================] - 1s 801us/step\n",
      "1387/1387 [==============================] - 1s 814us/step\n",
      "1387/1387 [==============================] - 1s 808us/step\n",
      "1387/1387 [==============================] - 1s 786us/step\n",
      "1387/1387 [==============================] - 1s 782us/step\n",
      "1387/1387 [==============================] - 1s 767us/step\n",
      "1387/1387 [==============================] - 1s 703us/step\n",
      "1387/1387 [==============================] - 1s 751us/step\n",
      "1387/1387 [==============================] - 1s 795us/step\n",
      "1387/1387 [==============================] - 1s 767us/step\n",
      "1387/1387 [==============================] - 1s 818us/step\n",
      "1387/1387 [==============================] - 1s 758us/step\n",
      "1387/1387 [==============================] - 1s 810us/step\n",
      "1387/1387 [==============================] - 1s 406us/step\n",
      "1387/1387 [==============================] - 1s 484us/step\n",
      "1387/1387 [==============================] - 1s 602us/step\n",
      "1387/1387 [==============================] - 1s 787us/step\n",
      "1387/1387 [==============================] - 1s 516us/step\n",
      "1387/1387 [==============================] - 1s 745us/step\n",
      "1387/1387 [==============================] - 1s 768us/step\n",
      "1387/1387 [==============================] - 1s 540us/step\n",
      "1387/1387 [==============================] - 1s 509us/step\n",
      "1387/1387 [==============================] - 1s 742us/step\n",
      "1387/1387 [==============================] - 1s 778us/step\n",
      "1387/1387 [==============================] - 1s 721us/step\n",
      "1387/1387 [==============================] - 1s 759us/step\n",
      "1387/1387 [==============================] - 1s 734us/step\n",
      "1387/1387 [==============================] - 1s 751us/step\n",
      "1387/1387 [==============================] - 1s 696us/step\n",
      "1387/1387 [==============================] - 1s 715us/step\n",
      "1387/1387 [==============================] - 1s 769us/step\n",
      "1387/1387 [==============================] - 1s 529us/step\n",
      "1387/1387 [==============================] - 1s 841us/step\n",
      "1387/1387 [==============================] - 1s 901us/step\n",
      "1387/1387 [==============================] - 1s 623us/step\n",
      "1387/1387 [==============================] - 1s 883us/step\n",
      "1387/1387 [==============================] - 1s 812us/step\n",
      "1387/1387 [==============================] - 1s 825us/step\n",
      "1387/1387 [==============================] - 1s 560us/step\n",
      "1387/1387 [==============================] - 1s 901us/step\n",
      "1387/1387 [==============================] - 1s 876us/step\n",
      "1387/1387 [==============================] - 1s 847us/step\n",
      "1387/1387 [==============================] - 1s 630us/step\n",
      "1387/1387 [==============================] - 1s 655us/step\n",
      "1387/1387 [==============================] - 1s 620us/step\n",
      "1387/1387 [==============================] - 1s 544us/step\n",
      "1387/1387 [==============================] - 1s 616us/step\n"
     ]
    }
   ],
   "source": [
    "acc_score=[]\n",
    "for f in [32,64]:\n",
    "  for f1 in [16,32]:\n",
    "    for k in [5,3]:\n",
    "      for k1 in [5,3]:\n",
    "        for d in [0.5,0.6]:\n",
    "          for p in [2,1]:\n",
    "            for s in [2,1]:\n",
    "\n",
    "              model = Sequential()\n",
    "              model.add(Conv1D(filters=f, kernel_size=k,padding='same', activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "              model.add(Conv1D(filters=f1, kernel_size=k1,padding='same', activation='relu',kernel_initializer='he_uniform'))\n",
    "              model.add(Dropout(d))\n",
    "              model.add(MaxPooling1D(pool_size=p,strides=s))\n",
    "\n",
    "              model.add(Flatten())\n",
    "              model.add(Dense(64, activation='relu',kernel_initializer='he_uniform'))\n",
    "              model.add(BatchNormalization()) \n",
    "              model.add(Dropout(d))\n",
    "\n",
    "              model.add(Dense(32, activation='relu',kernel_initializer='he_uniform'))\n",
    "              model.add(BatchNormalization()) \n",
    "              model.add(Dropout(d))\n",
    "\n",
    "              model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "              model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "              model.fit(X_train_dyn,y_train_dyn, epochs=10, batch_size=16,validation_data=(X_test_dyn, y_test_dyn),verbose=0)\n",
    "\n",
    "              #Evaluate the model \n",
    "              score = model.evaluate(X_test_dyn, y_test_dyn)\n",
    "\n",
    "              #Add the evaluation to a list \n",
    "              acc_score.append([f,f1,k,k1,d,p,s,score[1]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CtxwKgoU-uV3",
    "outputId": "1a96a97a-ae01-4c97-ce60-c8893e498740"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_1</th>\n",
       "      <th>conv_2</th>\n",
       "      <th>kernel1</th>\n",
       "      <th>kernel2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>poolsize</th>\n",
       "      <th>stride</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.932228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.956020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.975487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.979813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.945926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.964672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.965393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.944484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.950973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.959625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.956741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.954578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.961788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.974045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.967556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.942322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.926460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.916366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.968277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.933670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.946647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.953857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.969719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.927181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.932949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.984859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.974766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.967556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.966114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.939438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.966835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.932228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.972603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.943043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.980534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.900505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.937996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.922855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.971882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.959625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.953857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.951694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.970440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.865898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.966114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.956741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.904110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.945205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.937275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.941601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     conv_1  conv_2  kernel1  kernel2  dropout  poolsize  stride  accuracy\n",
       "0        32      16        5        5      0.5         2       2  0.932228\n",
       "1        32      16        5        5      0.5         2       1  0.963230\n",
       "2        32      16        5        5      0.5         1       2  0.956020\n",
       "3        32      16        5        5      0.5         1       1  0.975487\n",
       "4        32      16        5        5      0.6         2       2  0.979813\n",
       "5        32      16        5        5      0.6         2       1  0.945926\n",
       "6        32      16        5        5      0.6         1       2  0.964672\n",
       "7        32      16        5        5      0.6         1       1  0.965393\n",
       "8        32      16        5        3      0.5         2       2  0.944484\n",
       "9        32      16        5        3      0.5         2       1  0.963951\n",
       "10       32      16        5        3      0.5         1       2  0.950973\n",
       "11       32      16        5        3      0.5         1       1  0.959625\n",
       "12       32      16        5        3      0.6         2       2  0.956741\n",
       "13       32      16        5        3      0.6         2       1  0.963951\n",
       "14       32      16        5        3      0.6         1       2  0.954578\n",
       "15       32      16        5        3      0.6         1       1  0.961788\n",
       "16       32      16        3        5      0.5         2       2  0.974045\n",
       "17       32      16        3        5      0.5         2       1  0.967556\n",
       "18       32      16        3        5      0.5         1       2  0.942322\n",
       "19       32      16        3        5      0.5         1       1  0.963230\n",
       "20       32      16        3        5      0.6         2       2  0.926460\n",
       "21       32      16        3        5      0.6         2       1  0.916366\n",
       "22       32      16        3        5      0.6         1       2  0.968277\n",
       "23       32      16        3        5      0.6         1       1  0.933670\n",
       "24       32      16        3        3      0.5         2       2  0.946647\n",
       "25       32      16        3        3      0.5         2       1  0.925018\n",
       "26       32      16        3        3      0.5         1       2  0.953857\n",
       "27       32      16        3        3      0.5         1       1  0.969719\n",
       "28       32      16        3        3      0.6         2       2  0.927181\n",
       "29       32      16        3        3      0.6         2       1  0.932949\n",
       "..      ...     ...      ...      ...      ...       ...     ...       ...\n",
       "98       64      32        5        5      0.5         1       2  0.947368\n",
       "99       64      32        5        5      0.5         1       1  0.984859\n",
       "100      64      32        5        5      0.6         2       2  0.974766\n",
       "101      64      32        5        5      0.6         2       1  0.967556\n",
       "102      64      32        5        5      0.6         1       2  0.966114\n",
       "103      64      32        5        5      0.6         1       1  0.939438\n",
       "104      64      32        5        3      0.5         2       2  0.966835\n",
       "105      64      32        5        3      0.5         2       1  0.932228\n",
       "106      64      32        5        3      0.5         1       2  0.972603\n",
       "107      64      32        5        3      0.5         1       1  0.943043\n",
       "108      64      32        5        3      0.6         2       2  0.980534\n",
       "109      64      32        5        3      0.6         2       1  0.900505\n",
       "110      64      32        5        3      0.6         1       2  0.937996\n",
       "111      64      32        5        3      0.6         1       1  0.978371\n",
       "112      64      32        3        5      0.5         2       2  0.922855\n",
       "113      64      32        3        5      0.5         2       1  0.971882\n",
       "114      64      32        3        5      0.5         1       2  0.959625\n",
       "115      64      32        3        5      0.5         1       1  0.963951\n",
       "116      64      32        3        5      0.6         2       2  0.953857\n",
       "117      64      32        3        5      0.6         2       1  0.951694\n",
       "118      64      32        3        5      0.6         1       2  0.970440\n",
       "119      64      32        3        5      0.6         1       1  0.963951\n",
       "120      64      32        3        3      0.5         2       2  0.865898\n",
       "121      64      32        3        3      0.5         2       1  0.966114\n",
       "122      64      32        3        3      0.5         1       2  0.956741\n",
       "123      64      32        3        3      0.5         1       1  0.904110\n",
       "124      64      32        3        3      0.6         2       2  0.945205\n",
       "125      64      32        3        3      0.6         2       1  0.954578\n",
       "126      64      32        3        3      0.6         1       2  0.937275\n",
       "127      64      32        3        3      0.6         1       1  0.941601\n",
       "\n",
       "[128 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(acc_score,columns=['conv_1','conv_2','kernel1','kernel2','dropout','poolsize','stride','accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "_liKe0Rw_PCz",
    "outputId": "52059a90-6b40-4e56-c2bc-96890b9fd146"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_1</th>\n",
       "      <th>conv_2</th>\n",
       "      <th>kernel1</th>\n",
       "      <th>kernel2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>poolsize</th>\n",
       "      <th>stride</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.984859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    conv_1  conv_2  kernel1  kernel2  dropout  poolsize  stride  accuracy\n",
       "99      64      32        5        5      0.5         1       1  0.984859"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['accuracy']==df['accuracy'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FJg36DrT_US-",
    "outputId": "95f44fa4-91db-4698-8b93-d46bc17afbfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3285 samples, validate on 1387 samples\n",
      "Epoch 1/40\n",
      "3285/3285 [==============================] - 5s 1ms/step - loss: 1.2275 - acc: 0.5556 - val_loss: 0.4642 - val_acc: 0.8356\n",
      "Epoch 2/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.4150 - acc: 0.8524 - val_loss: 0.2090 - val_acc: 0.9373\n",
      "Epoch 3/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.3043 - acc: 0.9008 - val_loss: 0.1678 - val_acc: 0.9474\n",
      "Epoch 4/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.2123 - acc: 0.9306 - val_loss: 0.5599 - val_acc: 0.8327\n",
      "Epoch 5/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.1966 - acc: 0.9321 - val_loss: 0.2233 - val_acc: 0.9272\n",
      "Epoch 6/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.1777 - acc: 0.9412 - val_loss: 0.2448 - val_acc: 0.9373\n",
      "Epoch 7/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.1429 - acc: 0.9559 - val_loss: 0.1409 - val_acc: 0.9553\n",
      "Epoch 8/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.1691 - acc: 0.9452 - val_loss: 0.1068 - val_acc: 0.9683\n",
      "Epoch 9/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.1414 - acc: 0.9562 - val_loss: 0.0645 - val_acc: 0.9784\n",
      "Epoch 10/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.1082 - acc: 0.9656 - val_loss: 0.0914 - val_acc: 0.9661\n",
      "Epoch 11/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.1326 - acc: 0.9595 - val_loss: 0.0748 - val_acc: 0.9798\n",
      "Epoch 12/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0899 - acc: 0.9699 - val_loss: 0.1035 - val_acc: 0.9755\n",
      "Epoch 13/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.1050 - acc: 0.9653 - val_loss: 0.0825 - val_acc: 0.9813\n",
      "Epoch 14/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.1004 - acc: 0.9693 - val_loss: 0.1590 - val_acc: 0.9654\n",
      "Epoch 15/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.1160 - acc: 0.9671 - val_loss: 0.1335 - val_acc: 0.9733\n",
      "Epoch 16/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.1042 - acc: 0.9705 - val_loss: 0.1849 - val_acc: 0.9546\n",
      "Epoch 17/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0791 - acc: 0.9760 - val_loss: 0.2482 - val_acc: 0.9409\n",
      "Epoch 18/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0965 - acc: 0.9693 - val_loss: 0.0737 - val_acc: 0.9791\n",
      "Epoch 19/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0767 - acc: 0.9781 - val_loss: 0.1017 - val_acc: 0.9784\n",
      "Epoch 20/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0880 - acc: 0.9717 - val_loss: 0.0757 - val_acc: 0.9820\n",
      "Epoch 21/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0674 - acc: 0.9830 - val_loss: 0.0956 - val_acc: 0.9776\n",
      "Epoch 22/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0953 - acc: 0.9711 - val_loss: 0.1463 - val_acc: 0.9676\n",
      "Epoch 23/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0700 - acc: 0.9802 - val_loss: 0.0943 - val_acc: 0.9726\n",
      "Epoch 24/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0564 - acc: 0.9817 - val_loss: 0.0705 - val_acc: 0.9805\n",
      "Epoch 25/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0782 - acc: 0.9772 - val_loss: 0.1966 - val_acc: 0.9611\n",
      "Epoch 26/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0564 - acc: 0.9839 - val_loss: 0.1518 - val_acc: 0.9748\n",
      "Epoch 27/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0733 - acc: 0.9796 - val_loss: 0.1358 - val_acc: 0.9769\n",
      "Epoch 28/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0615 - acc: 0.9802 - val_loss: 0.1412 - val_acc: 0.9776\n",
      "Epoch 29/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0719 - acc: 0.9802 - val_loss: 0.1365 - val_acc: 0.9776\n",
      "Epoch 30/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0594 - acc: 0.9839 - val_loss: 0.1009 - val_acc: 0.9769\n",
      "Epoch 31/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0791 - acc: 0.9775 - val_loss: 0.1975 - val_acc: 0.9582\n",
      "Epoch 32/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0481 - acc: 0.9848 - val_loss: 0.1874 - val_acc: 0.9596\n",
      "Epoch 33/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0513 - acc: 0.9817 - val_loss: 0.1603 - val_acc: 0.9654\n",
      "Epoch 34/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0462 - acc: 0.9854 - val_loss: 0.1350 - val_acc: 0.9719\n",
      "Epoch 35/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0526 - acc: 0.9881 - val_loss: 0.1433 - val_acc: 0.9748\n",
      "Epoch 36/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0536 - acc: 0.9860 - val_loss: 0.1341 - val_acc: 0.9690\n",
      "Epoch 37/40\n",
      "3285/3285 [==============================] - 3s 1ms/step - loss: 0.0978 - acc: 0.9790 - val_loss: 0.2170 - val_acc: 0.9575\n",
      "Epoch 38/40\n",
      "3285/3285 [==============================] - 3s 997us/step - loss: 0.0423 - acc: 0.9887 - val_loss: 0.2426 - val_acc: 0.9488\n",
      "Epoch 39/40\n",
      "3285/3285 [==============================] - 3s 996us/step - loss: 0.1442 - acc: 0.9553 - val_loss: 0.2349 - val_acc: 0.9438\n",
      "Epoch 40/40\n",
      "3285/3285 [==============================] - 3s 992us/step - loss: 0.1417 - acc: 0.9559 - val_loss: 0.1208 - val_acc: 0.9776\n",
      "1387/1387 [==============================] - 0s 71us/step\n"
     ]
    }
   ],
   "source": [
    "  model = Sequential()\n",
    "  model.add(Conv1D(filters=64, kernel_size=5,padding='same', activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "  model.add(Conv1D(filters=32, kernel_size=5,padding='same', activation='relu',kernel_initializer='he_uniform'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(MaxPooling1D(pool_size=1,strides=1))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(64, activation='relu',kernel_initializer='he_uniform'))\n",
    "  model.add(BatchNormalization()) \n",
    "  model.add(Dropout(0.5))\n",
    "\n",
    "  model.add(Dense(32, activation='relu',kernel_initializer='he_uniform'))\n",
    "  model.add(BatchNormalization()) \n",
    "  model.add(Dropout(0.5))\n",
    "\n",
    "  model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  model.fit(X_train_dyn,y_train_dyn, epochs=40, batch_size=8,validation_data=(X_test_dyn, y_test_dyn),verbose=1)\n",
    "\n",
    "  #Evaluate the model \n",
    "  score = model.evaluate(X_test_dyn, y_test_dyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DJd_KGgLPSK"
   },
   "outputs": [],
   "source": [
    "model.save('drive/My Drive/Data/Human_activity_recognition/model_dyn_class.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yHXrq_J2uDy6"
   },
   "source": [
    "## Final Prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "dhQdBLkfuKHT",
    "outputId": "3eedab98-a5a6-4402-af58-3b58405aa364"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0701 04:13:50.674447 140289857959808 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0701 04:13:53.888343 140289857959808 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0701 04:13:54.012605 140289857959808 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0701 04:13:54.128575 140289857959808 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "model_2class = load_model('drive/My Drive/Data/Human_activity_recognition/model_2class.h5')\n",
    "model_dynamic = load_model('drive/My Drive/Data/Human_activity_recognition/model_stat_class.h5')\n",
    "model_static = load_model('drive/My Drive/Data/Human_activity_recognition/model_dyn_class.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRaKr6yYwKDb"
   },
   "source": [
    "## Two class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XzavYfjivStt"
   },
   "outputs": [],
   "source": [
    " ##predicting whether dynamic or static\n",
    "predict_2class = model_2class.predict(X_test)\n",
    "Y_pred_2class =  np.argmax(predict_2class, axis=1)\n",
    "#static data filter\n",
    "X_static = X_test[Y_pred_2class==1]\n",
    "#dynamic data filter\n",
    "X_dynamic = X_test[Y_pred_2class==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MoGl0JlTv8rJ"
   },
   "outputs": [],
   "source": [
    "#predicting static activities\n",
    "predict_static = model_static.predict(X_train_stat)\n",
    "predict_static = np.argmax(predict_static,axis=1)\n",
    "predict_static = predict_static + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-46yoDN3kjX"
   },
   "outputs": [],
   "source": [
    "#predicting dynamic activites\n",
    "predict_dynamic = model_dynamic.predict(X_train_dyn)\n",
    "predict_dynamic = np.argmax(predict_dynamic,axis=1)\n",
    "predict_dynamic = predict_dynamic + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5QnKKUu363w"
   },
   "outputs": [],
   "source": [
    "#appending final output to one list in the same sequence of input data\n",
    "i,j = 0,0 \n",
    "final_pred = []\n",
    "for mask in Y_pred_2class:\n",
    "    if mask == 1:\n",
    "        final_pred.append(predict_static[i])\n",
    "        i = i + 1\n",
    "    else:\n",
    "        final_pred.append(predict_dynamic[j])\n",
    "        j = j + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "jOOq8XNX44Ao",
    "outputId": "d51085de-42ba-4cf9-ec30-de5da8b5e35c"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-73c6b20be5f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy of train data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and multiclass targets"
     ]
    }
   ],
   "source": [
    "##accuracy of test\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy of train data',accuracy_score(y_test,final_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "zyIazTkw4v4m",
    "outputId": "6295e514-c1a9-4e2f-b384-e19d7d10f47a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-448e7c991ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WALKING'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'WALKING_UPSTAIRS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'WALKING_DOWNSTAIRS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SITTING'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'STANDING'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'LAYING'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m plot_confusion_matrix(cm, classes=labels, \n\u001b[0m\u001b[1;32m      4\u001b[0m                       normalize=True, title='Normalized confusion matrix', cmap = plt.cm.Greens)\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x576 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "labels=['WALKING','WALKING_UPSTAIRS','WALKING_DOWNSTAIRS','SITTING','STANDING','LAYING']\n",
    "plot_confusion_matrix(cm, classes=labels, \n",
    "                      normalize=True, title='Normalized confusion matrix', cmap = plt.cm.Greens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mrvlVdyl43Hc",
    "outputId": "ec85c7cb-509c-4bc3-d048-8e88c7001f5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_UygqQA402L"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ConquerDivide_HAR_EDA",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
